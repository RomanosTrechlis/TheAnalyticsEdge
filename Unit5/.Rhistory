Sys.setlocale("LC_ALL", "C")
View(df)
8*6
2^16
2^
16
2^
8*10
sqrt(4)
sqrt(2)
?sqrt
SquareRoot2 = sqrt(2)
SquareRoot2
SquareRoot2
squareRoot2
HoursYear <- 365*24
HoursYear
ls()
c(2,3,5,8,13)
Country = c("Brazil","China","India","Switcherland", "USA")
LifeExpectancy = c(74,76,65,83,79)
Country
LifeExpectancy
ls()
Country[1]
LifeExpectancy[3]
seq(0,100,2)
#dataframes
CountryData = data.frame(Country, LifeExpectancy)
CountryData
CountryData$Population = c(199000, 1390000, 12400000, 7997, 318000)
CountryData
Country = c("Australia", "Greece")
LifeExpectancy = c(82,81)
Population - c(23050,11125)
Population = c(23050,11125)
NewCountryData = data.frame(Country, LifeExpectancy, Population)
NewCountryData
AllCountryData = rbind(CountryData, NewCountryData)
AllCountryData
summary(AllCountryData)
str(AllCountryData$Population)
str(AllCountryData)
install.packages("tm")
install.packages("SnowballC")
setwd("C:\\Users\\xa86\\Desktop\\Unit5")
tweets = read.csv("tweets.csv", stringsAsFactors=FALSE)
tweets$Negative = as.factor(tweets$Avg <= -1)
library(tm)
library(SnowballC)
corpus = Corpus(VectorSource(tweets$Tweet))
corpus[[1]]
corpus = tm_map(corpus, PlainTextDocument)
corpus = tm_map(corpus, removePunctuation)
corpus[[1]]
# Look at stop words
stopwords("english")[1:10]
# Remove stopwords and apple
corpus = tm_map(corpus, removeWords, c("apple", stopwords("english")))
corpus[[1]]
# Stem document
corpus = tm_map(corpus, stemDocument)
corpus[[1]]
# Video 6
# Create matrix
frequencies = DocumentTermMatrix(corpus)
frequencies
inspect(frequencies[1000:1005,505:515])
findFreqTerms(frequencies, lowfreq=20)
findFreqTerms(frequencies, lowfreq=100)
corpus = tm_map(corpus, removeWords, c("apple", stopwords("english")))
corpus = tm_map(corpus, stemDocument)
frequencies = DocumentTermMatrix(corpus)
frequencies
inspect(frequencies[1000:1005,505:515])
findFreqTerms(frequencies, lowfreq=20)
corpus = Corpus(VectorSource(tweets$Tweet))
corpus = tm_map(corpus, PlainTextDocument)
corpus = tm_map(corpus, tolower)
corpus = tm_map(corpus, removePunctuation)
corpus = tm_map(corpus, removeWords, c("apple", stopwords("english")))
corpus = tm_map(corpus, stemDocument)
frequencies = DocumentTermMatrix(corpus)
inspect(frequencies[1000:1005,505:515])
findFreqTerms(frequencies, lowfreq=20)
findFreqTerms(frequencies, lowfreq=100)
length(stopwords("english"))
Sys.setlocale("LC_ALL", "C")
tweets = read.csv("tweets.csv", stringsAsFactors=FALSE)
tweets$Negative = as.factor(tweets$Avg <= -1)
corpus = Corpus(VectorSource(tweets$Tweet))
corpus = tm_map(corpus, PlainTextDocument)
corpus = tm_map(corpus, tolower)
corpus[[1]]
corpus = tm_map(corpus, PlainTextDocument)
corpus[[1]]
corpus = tm_map(corpus, removePunctuation)
corpus[[1]]
corpus = tm_map(corpus, tolower)
corpus[[1]]
corpus = tm_map(corpus, removePunctuation)
corpus[[1]]
corpus = tm_map(corpus, removeWords, c("apple", stopwords("english")))
corpus[[1]]
corpus = tm_map(corpus, stemDocument)
corpus[[1]]
frequencies = DocumentTermMatrix(corpus)
corpus = tm_map(corpus, PlainTextDocument)
corpus = tm_map(corpus, stemDocument)
corpus[[1]]
frequencies = DocumentTermMatrix(corpus)
inspect(frequencies[1000:1005,505:515])
findFreqTerms(frequencies, lowfreq=20)
findFreqTerms(frequencies, lowfreq=100)
sparse = removeSparseTerms(frequencies, 0.995)
sparse
tweetsSparse = as.data.frame(as.matrix(sparse))
# Make all variable names R-friendly
colnames(tweetsSparse) = make.names(colnames(tweetsSparse))
# Add dependent variable
tweetsSparse$Negative = tweets$Negative
# Split the data
library(caTools)
set.seed(123)
split = sample.split(tweetsSparse$Negative, SplitRatio = 0.7)
trainSparse = subset(tweetsSparse, split==TRUE)
testSparse = subset(tweetsSparse, split==FALSE)
# Video 7
# Build a CART model
library(rpart)
library(rpart.plot)
tweetCART = rpart(Negative ~ ., data=trainSparse, method="class")
prp(tweetCART)
predictCART = predict(tweetCART, newdata=testSparse, type="class")
table(testSparse$Negative, predictCART)
# Compute accuracy
(294+18)/(294+6+37+18)
table(testSparse$Negative)
300/(300+55)
library(randomForest)
set.seed(123)
tweetRF = randomForest(Negative ~ ., data=trainSparse)
predictRF = predict(tweetRF, newdata=testSparse)
table(testSparse$Negative, predictRF)
(293+21)/(293+7+34+21)
tweetLog = glm(Negative ~ ., data=trainSparse, family="binomial")
predictions = predict(tweetLog, newdata=testSparse, type="response")
table(testSparse$Negative, predictions)
(259+31)/(259+31+41+24)
wiki = read.csv("wiki.csv", stringsAsFactors=FALSE)
wiki$Vandal = as.factor(wiki$Vandal)
table(wiki$Vandal)
corpusAdded = Corpus(VectorSource(wiki$Added))
corpusAdded[[1]]
corpusAdded = tm_map(corpusAdded, removeWords, stopwords("english"))
corpusAdded[[1]]
corpusAdded = tm_map(corpusAdded, stemDocument)
corpusAdded[[1]]
dtmAdded = DocumentTermMatrix(corpusAdded)
dtmAdded
sparseAdded = removeSparseTerms(frequencies, 0.97)
sparseAdded
sparseAdded = removeSparseTerms(frequencies, 0.997)
sparseAdded
sparseAdded = removeSparseTerms(dtmAdded, 0.997)
sparseAdded
wordsAdded = as.data.frame(as.matrix(sparseAdded))
colnames(wordsAdded) = paste("A", colnames(wordsAdded))
corpusRemoved = Corpus(VectorSource(wiki$Removed))
corpusRemoved[[1]]
corpusRemoved = tm_map(corpusRemoved, removeWords, stopwords("english"))
corpusRemoved = tm_map(corpusRemoved, stemDocument)
dtmRemoved = DocumentTermMatrix(corpusRemoved)
sparseRemoved = removeSparseTerms(dtmAdded, 0.997)
wordsRemoved = as.data.frame(as.matrix(sparseRemoved))
str(wordsRemoved)
colnames(wordsRemoved) = paste("R", colnames(wordsRemoved))
str(wordsRemoved)
sparseRemoved
sparseAdded
corpusRemoved[[1]]
wiki = read.csv("wiki.csv", stringsAsFactors=FALSE)
wiki$Vandal = as.factor(wiki$Vandal)
corpusAdded = Corpus(VectorSource(wiki$Added))
corpusAdded = tm_map(corpusAdded, removeWords, stopwords("english"))
corpusAdded = tm_map(corpusAdded, stemDocument)
dtmAdded = DocumentTermMatrix(corpusAdded)
dtmAdded
sparseAdded = removeSparseTerms(dtmAdded, 0.997)
sparseAdded
wordsAdded = as.data.frame(as.matrix(sparseAdded))
colnames(wordsAdded) = paste("A", colnames(wordsAdded))
corpusRemoved = Corpus(VectorSource(wiki$Removed))
corpusRemoved = tm_map(corpusRemoved, removeWords, stopwords("english"))
corpusRemoved = tm_map(corpusRemoved, stemDocument)
corpusRemoved
dtmRemoved = DocumentTermMatrix(corpusRemoved)
dtmRemoved
sparseRemoved = removeSparseTerms(dtmRemoved, 0.997)
sparseRemoved
wordsRemoved = as.data.frame(as.matrix(sparseRemoved))
colnames(wordsRemoved) = paste("R", colnames(wordsRemoved))
str(wordsRemoved)
wikiWords = cbind(wordsAdded, wordsRemoved)
wikiWords$Vandal = wiki$Vandal
library(caTools)
set.seed(123)
split = sample.split(wikiWords$Vandal, SplitRatio = 0.7)
train = subset(wikiWords, split==TRUE)
test = subset(wikiWords, split==FALSE)
table(test$Vandal)
618/(618+545)
library(rpart)
library(rpart.plot)
modelCART = rpart(Vandal ~ ., data=train, type="class")
modelCART = rpart(Vandal ~ ., data=train, method="class")
predCART = predict(modelCART, newdata=test, type="class")
table(test$Vandal, predCART)
(618+12)/(618+12+533)
prp(modelCART)
predCART = predict(modelCART, type="class")
table(train$Vandal, predCART)
(1443+33)/(1443+33+1237)
predCART = predict(modelCART, newdata=test, type="class")
wikiWords2 = wikiWords
wikiWords2$HTTP = ifelse(grepl("http",wiki$Added,fixed=TRUE), 1, 0)
summary(wikiWords2$HTTP)
sum(wikiWords2$HTTP)
wikiTrain2 = subset(wikiWords2, spl==TRUE)
wikiTrain2 = subset(wikiWords2, split==TRUE)
wikiTest2 = subset(wikiWords2, split==FALSE)
modelCART2 = rpart(Vandal ~ HTTP, data=wikiTrain2, method="class")
prp(modelCART2)
predCART2 = predict(modelCART2, newdata=wikiTest2, type="class")
table(wikiTest2$Vandal, predCART2)
(609+45)/(609+45+9+500)
modelCART2 = rpart(Vandal ~ ., data=wikiTrain2, method="class")
prp(modelCART2)
predCART2 = predict(modelCART2, newdata=wikiTest2, type="class")
table(wikiTest2$Vandal, predCART2)
(609+57)/(609+57+9+488)
wikiWords2$NumWordsAdded = rowSums(as.matrix(dtmAdded))
wikiWords2$NumWordsRemoved = rowSums(as.matrix(dtmRemoved))
mean(wikiWords2$NumWordsAdded)
wikiTrain2 = subset(wikiWords2, split==TRUE)
wikiTest2 = subset(wikiWords2, split==FALSE)
modelCART2 = rpart(Vandal ~ ., data=wikiTrain2, method="class")
prp(modelCART2)
predCART2 = predict(modelCART2, newdata=wikiTest2, type="class")
table(wikiTest2$Vandal, predCART2)
(514+248)/(514+248+104+297)
wikiWords3 = wikiWords2
wikiWords3$Minor = wiki$Minor
wikiWords3$Loggedin = wiki$Loggedin
wikiTrain3 = subset(wikiWords3, split==TRUE)
wikiTest3 = subset(wikiWords3, split==FALSE)
modelCART3 = rpart(Vandal ~ ., data=wikiTrain3, method="class")
prp(modelCART3)
predCART2 = predict(modelCART3, newdata=wikiTest3, type="class")
table(wikiTest3$Vandal, predCART3)
predCART3 = predict(modelCART3, newdata=wikiTest3, type="class")
table(wikiTest3$Vandal, predCART3)
(595+241)/(595+241+23+304)
prp(modelCART3)
trials = read.csv("clinical_trial.csv", stringsAsFactors=FALSE)
str(trials)
max(nchar(trials$abstract))
nchar(trials$abstract)
nrow(subset(trials, abstract == ""))
min(nchar(trials$abstract))
subset(trials, min(nchar(abstract)))
subset(trials, title == "")
subset(trials, title == " ")
min(nchar(trials$title))
subset(trials, title == "28")
subset(trials, nchar(title)==28)
subset(trials, nchar(title)==28)$title
corpusTitle  = Corpus(VectorSource(trials$title))
corpusAbstract = Corpus(VectorSource(trials$abstract))
corpusTitle = tm_map(corpusTitle, tolower)
corpusTitle = tm_map(corpusTitle, PlainTextDocument)
corpusAbstract = tm_map(corpusAbstract, tolower)
corpusAbstract = tm_map(corpusAbstract, PlainTextDocument)
corpusTitle = tm_map(corpusTitle, removePunctuation)
corpusAbstract = tm_map(corpusAbstract, removePunctuation)
corpusTitle = tm_map(corpusTitle, removeWords, stopwords("english"))
corpusAbstract = tm_map(corpusAbstract, removeWords, stopwords("english"))
corpusTitle = tm_map(corpusTitle, stemDocument)
corpusAbstract = tm_map(corpusAbstract, stemDocument)
dtmTitle = DocumentTermMatrix(corpusTitle)
dtmAbstract = DocumentTermMatrix(corpusAbstract)
dtmTitle = removeSparseTerms(dtmTitle, 0.95)
dtmAbstract = removeSparseTerms(dtmAbstract, 0.95)
dtmTitle = as.data.frame(as.matrix(dtmTitle))
dtmAbstract = as.data.frame(as.matrix(dtmAbstract))
str(dtmTitle)
str(dtmAbstract)
colSums(dtmAbstract)
max(colSums(dtmAbstract))
which.max(colSums(dtmAbstract))
colnames(dtmTitle) = paste0("T", colnames(dtmTitle))
colnames(dtmAbstract) = paste0("A", colnames(dtmAbstract))
dtm = cbind(dtmTitle, dtmAbstract)
STR(DTM)
str(dtm)
dtm$trial = trials$trial
set.seed(144)
split = sample.split(dtm$trial, SplitRatio = 0.7)
train = subset(dtm, split==TRUE)
test = subset(dtm, split==FALSE)
table(train$trial)
730/(730+572)
trialCART = rpart(trial ~ ., data=train, mehtod="class")
trialCART = rpart(trial ~ ., data=train, method="class")
prp(trialCART)
trainPred = predict(trialCART)
str(trainPred)
pred.prob = trainPred[,2]
max(pred.prob)
confMatrix = confusionMatrix(pred.prob, train$trial)
library(caret)
install.packages("caret")
confMatrix = confusionMatrix(pred.prob, train$trial)
library(e1071)
library(caret)
install.packages("caret")
library(caret)
install.packages("ggplot2")
library(caret)
install.packages("e1071")
library(e1071)
confMatrix = confusionMatrix(pred.prob, train$trial)
library(randomForest)
confMatrix = confusionMatrix(pred.prob, train$trial)
install.packages("caret")
library(caret)
install.packages("stringi")
library(caret)
install.packages("pbkrtest")
table(train$trial, pred.prob >= 0.5)
(631+441)/(631+441+99+131)
(441+131)/(631+441+99+131)
(631+99)/(631+441+99+131)
(441+131)/(631+441+99+131)
table(train$trial, pred.prob >= 0.5)
(441)/(441+131)
(631)/(631+99)
testPred = predict(trialCART, newdate=test)
str(testPred)
pred.prob = testPred[,2]
table(test$trial, pred.prob >= 0.5)
testPred = predict(trialCART, newdate=test)
pred.prob = testPred[,2]
table(test$trial, pred.prob >= 0.5)
str(testPred)
str(test)
pred.prob = testPred[,2]
str(pred.prob)
pred.prob = data.frame(testPred)$X1
table(test$trial, pred.prob >= 0.5)
str(data.frame(testPred)$X1)
str(data.frame(testPred))
str(test)
testPred = predict(trialCART, newdata=test)
pred.prob = data.frame(testPred)$X1
table(test$trial, pred.prob >= 0.5)
(261+162)/(261+162+52+83)
library(ROCR)
install.packages("gplots")
predROCR = prediction(pred.prob, test$trial)
perfROCR = performance(predROCR, "tpr", "fpr")
plot(perfROCR, colorize=TRUE)
auc.perf = performance(predROCR, measure = "auc")
auc.perf@y.values
table(test$trial, pred.prob >= 0.5)
emails = read.csv("emails.csv", stringsAsFactors=FALSE)
nrow(emails)
str(emails)
sum(emails$spam)
max(nchar(emails$text))
which.min(nchar(emails$text))
corpus  = Corpus(VectorSource(emails$text))
corpus = tm_map(corpus, tolower)
corpus = tm_map(corpus, PlainTextDocument)
corpus = tm_map(corpus, removePunctuation)
corpus = tm_map(corpus, removeWords, stopwords("english"))
corpus = tm_map(corpus, stemDocument)
dtm = DocumentTermMatrix(corpus)
str(dtm)
dtm
spdtm  = removeSparseTerms(dtm, 0.95)
str(spdtm)
spdtm
emailsSparse  = as.data.frame(as.matrix(spdtm))
colnames(emailsSparse) = make.names(colnames(emailsSparse))
which.max(colSums(emailsSparse))
emailsSparse$spam = emails$spam
nrow(subset(emailsSparse, spam == 0 & colSums(emailsSparse)>5000))
ncol(subset(emailsSparse, spam == 0)) - 1
nrow(subset(emailsSparse, spam == 0 & colSums(emailsSparse) > 5000))
p = subset(emailsSparse, spam == 0)
colSums(p) > 5000
colSums(p)
str(colSums(p))
summary(colSums(p))
p[colSums(p) > 5000]
spl = colSums(p) > 5000
res = subset(p, spl==TRUE)
nrow(res)
spl
spl = colSums(p) >= 5000
res = subset(p, spl==TRUE)
nrow(res)
res
str(res)
colSums(emailsSparse)
str(colSums(emailsSparse))
data.frame(colSums(emailsSparse))
res = data.frame(colSums(emailsSparse))[colSums.emailsSparse.>=5000]
res = data.frame(colSums(emailsSparse))[colSums.emailsSparse>=5000]
res = data.frame(colSums(emailsSparse))
res[colSums.emailsSparse>=5000]
str(res)
res[colSums.emailsSparse. >= 5000]
res[res$colSums.emailsSparse. >= 5000]
res[res$colSums.emailsSparse >= 5000]
a = colSums(emailsSparse)
res = data.frame(a)
str(res)
res[res$a >= 5000]
res[res$a >= 5000,]
subset(res, a >= 5000)
sort(colSums(subset(emailsSparse, spam == 0)))
sort(colSums(subset(emailsSparse, spam == 1)))
emailsSparse$spam = as.factor(emailsSparse$spam)
set.seed(123)
split = sample.split(emailsSparse$spam, SplitRatio = 0.7)
train = subset(emailsSparse, split==TRUE)
test = subset(emailsSparse, split==FALSE)
table(emailsSparse$spam)
spamLog = glm(spam ~ ., data=train, family="binomial")
summary(spamLog)
spamCART = rpart(spam ~ ., data=train, method="class")
prp(spamCART)
set.seed(123)
spamRF = randomForest(spam ~ ., data=train)
logPred = predict(spamLog, type="response")
str(logPred)
logPred
logPred = predict(spamLog)
logPred
logPred = predict(spamLog, type="response")
data.frame(logPred)
spamCART = rpart(spam ~ ., data=train, method="class")
treePred = predict(spamCART)
treePred[,2]
pred.tree = treePred[,2]
pred.log = logPred
forestPred = predict(spamRF, type="prob")
pred.tree = forestPred[,2]
pred.tree = treePred[,2]
pred.forest = forestPred[,2]
pred.log < 0.00001
sum(pred.log < 0.00001)
sum(pred.log > 0.99999)
pred.log
sum(pred.log < 0.00001)
sum(pred.log > 0.99999)
sum(pred.log <= 0.99999 & pred.log >= 0.00001)
prp(spamCART)
table(train$spam, pred.log >= 0.5)
(3052+954)/(3052+954+4+0)
predROCR = prediction(pred.log, train$spam)
perfROCR = performance(predROCR, "tpr", "fpr")
plot(perfROCR, colorize=TRUE)
auc.perf = performance(predROCR, measure = "auc")
auc.perf@y.values
table(train$spam, pred.tree >= 0.5)
(2885+894)/(2885+894+167+64)
predROCR = prediction(pred.tree, train$spam)
perfROCR = performance(predROCR, "tpr", "fpr")
plot(perfROCR, colorize=TRUE)
auc.perf = performance(predROCR, measure = "auc")
auc.perf@y.values
table(train$spam, pred.forest >= 0.5)
(3013+914)/(3013+914+39+44)
predROCR = prediction(pred.forest, train$spam)
perfROCR = performance(predROCR, "tpr", "fpr")
plot(perfROCR, colorize=TRUE)
auc.perf = performance(predROCR, measure = "auc")
auc.perf@y.values
logPred = predict(spamLog, type="response", newdata=test)
predTest.log = logPred
treePred = predict(spamCART, newdata=test)
predTest.tree = treePred[,2]
forestPred = predict(spamRF, type="prob", newdata=test)
predTest.forest = forestPred[,2]
table(test$spam, predTest.log >= 0.5)
(1257+376)/(1257+376+51+34)
predROCR = prediction(predTest.log, test$spam)
auc.perf = performance(predROCR, measure = "auc")
auc.perf@y.values
table(test$spam, predTest.tree >= 0.5)
(1228+386)/(1228+386+80+24)
predROCR = prediction(predTest.tree, test$spam)
auc.perf = performance(predROCR, measure = "auc")
auc.perf@y.values
table(test$spam, predTest.forest >= 0.5)
(1290+386)/(1290+386+18+24)
predROCR = prediction(predTest.forest, test$spam)
auc.perf = performance(predROCR, measure = "auc")
auc.perf@y.values
